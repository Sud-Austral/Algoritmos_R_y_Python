---
title: "PCA"
author: "Christian Castro"
date: "13-07-2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/
## PCA

El análisis de componentes principales (PCA) nos permite resumir y visualizar información de un conjunto de datos de individuos u observaciones descritos por múltiples variables cuantitativas interrelacionadas. Cada variable podría considerarse como una dimensión diferente. Si tiene más de 3 variables en sus conjuntos de datos, podría ser muy difícil visualizar un hiperespacio multidimensional.

El análisis de componentes principales se utiliza para extraer la información importante de una tabla de datos multivariada y para expresar esta información como un nuevo conjunto reducido de nuevas variables llamadas componentes principales. Estas nuevas variables corresponden a una combinación lineal de los originales. El número de componentes principales es menor o igual que el número de variables originales.

La información en un conjunto de datos dado corresponde a la variación total que contiene. El objetivo de PCA es identificar direcciones (o componentes principales) a lo largo de los cuales la variación en los datos es máxima.

En otras palabras, PCA reduce la dimensionalidad de los datos multivariados a dos o tres componentes principales, que pueden visualizarse gráficamente, con una pérdida mínima de información.

## Introduccion

Comprender los detalles de PCA requiere conocimientos de álgebra lineal.

En la gráfica 1A a continuación, los datos se representan en el sistema de coordenadas X-Y. La reducción de la dimensión se logra mediante la identificación de las direcciones principales, llamadas componentes principales, en las que los datos varían.

PCA supone que las direcciones con las variaciones más grandes son las más "importantes" (es decir, las más principales).

En la figura siguiente, el eje PC1 es la primera dirección principal a lo largo de la cual las muestras muestran la mayor variación. El eje PC2 es la segunda dirección más importante y es ortogonal al eje PC1.

La dimensionalidad de nuestros datos bidimensionales se puede reducir a una sola dimensión proyectando cada muestra en el primer componente principal (Gráfico 1B)

Técnicamente hablando, la cantidad de varianza retenida por cada componente principal se mide por el llamado valor propio (eigenvalue).

El método PCA es particularmente útil cuando las variables dentro del conjunto de datos están altamente correlacionadas. La correlación indica que hay redundancia en los datos. Debido a esta redundancia, PCA puede usarse para reducir las variables originales en un número menor de nuevas variables (= componentes principales) que explican la mayor parte de la varianza en las variables originales.

En conjunto, el objetivo principal del análisis de componentes principales es:

     identificar patrones ocultos en un conjunto de datos,
     reducir la dimensionalidad de los datos eliminando el ruido y la redundancia en los datos,
     identificar variables correlacionadas

```{r cars}

library("FactoMineR")
library("factoextra")
library(readxl)
library(XLConnect)
library("openxlsx")
library(httr)
library(xlsx)
library(readr)

# para leer un repositorio privado:
# github_link <- "https://raw.githubusercontent.com/Sud-Austral/Bases_de_Datos_DS/master/DMCS/Tasa%20Anual_DENUNCIA_Robo_con_vio.csv"
# api_key <- "1913b82cc62967f3e5fe93e9ce176c806e0703ec"
# req <- GET(github_link, add_headers(Authorization = paste("token", api_key)))
#############################

# para llenar un pepositorio: tiene un error.

# data <- read_csv( "https://raw.githubusercontent.com/Sud-Austral/Bases_de_Datos_DS/master/DMCS/Tasa%20Anual_DENUNCIA_Robo_con_vio.csv")
# write.csv(data,'data.csv')
#############################

delitos_2019 <- read.csv("C:/Users/usuario/Documents/GitHub_Personal/Bases_de_Datos_DS/tasa_2019_den_bueno.csv",
  header = TRUE,
  stringsAsFactors = FALSE, 
  strip.white = TRUE,
  sep = ';', 
  dec = ",",
  row.names=1)

delitos_2019$robo_con_vio <- as.numeric(delitos_2019$robo_con_vio)
delitos_2019$robo_con_sor <- as.numeric(delitos_2019$robo_con_sor)
delitos_2019$robo_con_fuer <- as.numeric(delitos_2019$robo_con_fuer)
delitos_2019$robo_vehi <- as.numeric(delitos_2019$robo_vehi)
delitos_2019$robo_acce_vehi <- as.numeric(delitos_2019$robo_acce_vehi)
delitos_2019$robo_lugar_h <- as.numeric(delitos_2019$robo_lugar_h)
delitos_2019$robo_lugar_nh <- as.numeric(delitos_2019$robo_lugar_nh)

delitos_2019$robos_con_fuer_otros <- as.numeric(delitos_2019$robos_con_fuer_otros )


delitos_2019$hurtos<- as.numeric(delitos_2019$hurtos)
delitos_2019$lesiones <- as.numeric(delitos_2019$lesiones)
delitos_2019$lesiones_lev <- as.numeric(delitos_2019$lesiones_lev)
delitos_2019$lesiones_gra <- as.numeric(delitos_2019$lesiones_gra)


delitos_2019$violacion <- as.numeric(delitos_2019$violacion)
delitos_2019$homicidio <- as.numeric(delitos_2019$homicidio)


typeof(delitos_2019$robo_con_vio)
delitos_2019.active <- na.omit(delitos_2019)

head(delitos_2019.active)
 res.pca <- PCA(delitos_2019.active, graph = FALSE)
 print(res.pca)
eig.val <- get_eigenvalue(res.pca)
eig.val
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))
```



```{r cars 10}

data(decathlon2)
head(decathlon2)

# Comenzamos extrayendo el subconjunto de individuos activos y variables activas para el análisis del componente principal:
decathlon2.active <- decathlon2[1:23, 1:10]
head(decathlon2.active[, 1:6], 4)

# Estandarizacion de los datos
#La función de base R `scale() se puede utilizar para estandarizar los datos. Toma una matriz numérica como entrada y realiza el escalado en las columnas.

#Tenga en cuenta que, por defecto, la función PCA () [en FactoMineR], estandariza los datos automáticamente durante el PCA; así que no necesita hacer esta transformación antes del PCA.

# PCA(X, scale.unit = TRUE, ncp = 5, graph = TRUE)

# Donde:
# X: un dataframe. Las filas son individuos y las columnas son variables numéricas.
# scale.unit: un valor lógico. Si es VERDADERO, los datos se escalan a la varianza de la unidad antes del análisis. Esta estandarización a la misma escala evita que algunas variables se vuelvan dominantes solo por sus grandes unidades de medida. Hace variable comparable.
# ncp: número de dimensiones mantenidas en los resultados finales.
# graph: un valor lógico. Si es VERDADERO, se muestra un gráfico.

# El siguiente código R calcula el análisis de componentes principales en los individuos / variables activos:


res.pca <- PCA(decathlon2.active, graph = FALSE)

# El resultado de la función PCA() es una lista que incluye los siguientes componentes: análisis de los individuos / variables activos:
print(res.pca)
```

Visualización e interpretación

Utilizaremos el paquete factoextra R para ayudar en la interpretación de PCA. 

Estas funciones incluyen:

    get_eigenvalue (res.pca): extrae los valores propios / varianzas de los componentes principales
    fviz_eig (res.pca): visualiza los valores propios
    get_pca_ind (res.pca), get_pca_var (res.pca): extrae los resultados para individuos y variables, respectivamente.
    fviz_pca_ind (res.pca), fviz_pca_var (res.pca): visualice los resultados individuales y variables, respectivamente.
    fviz_pca_biplot (res.pca): crea un biplot de individuos y variables.

En las siguientes secciones, ilustraremos cada una de estas funciones.

Valores propios / variaciones

Como se describió en secciones anteriores, los valores propios miden la cantidad de variación retenida por cada componente principal. Los valores propios son grandes para las primeras PC y pequeños para las PC posteriores. Es decir, las primeras PC corresponden a las direcciones con la cantidad máxima de variación en el conjunto de datos.

Examinamos los valores propios para determinar el número de componentes principales a considerar. Los valores propios y la proporción de variaciones (es decir, información) retenidos por los componentes principales (PC) se pueden extraer mediante la función get_eigenvalue () [paquete factoextra].


```{r cars3}

eig.val <- get_eigenvalue(res.pca)
eig.val
```

La suma de todos los valores propios da una varianza total de 10.

La proporción de variación explicada por cada valor propio se da en la segunda columna. Por ejemplo, 4.124 dividido por 10 es igual a 0.4124, o aproximadamente el 41.24% de la variación se explica por este primer valor propio. El porcentaje acumulado explicado se obtiene sumando las proporciones sucesivas de variación explicadas para obtener el total acumulado. Por ejemplo, 41.242% más 18.385% es igual a 59.627%, y así sucesivamente. Por lo tanto, aproximadamente el 59.627% de la variación se explica por los dos primeros valores propios juntos.

Los valores propios se pueden usar para determinar el número de componentes principales a retener después de PCA (Kaiser 1961):

    Un valor propio> 1 indica que las PC representan más varianza que la contabilizada por una de las variables originales en los datos estandarizados. Esto se usa comúnmente como un punto de corte para el cual se retienen las PC. Esto es válido solo cuando los datos están estandarizados.

    También puede limitar el número de componentes a ese número que representa una cierta fracción de la varianza total. Por ejemplo, si está satisfecho con el 70% de la varianza total explicada, use el número de componentes para lograrlo.

Desafortunadamente, no hay una forma objetiva bien aceptada de decidir cuántos componentes principales son suficientes. Esto dependerá del campo de aplicación específico y del conjunto de datos específico. En la práctica, tendemos a mirar los primeros componentes principales para encontrar patrones interesantes en los datos.

En nuestro análisis, los primeros tres componentes principales explican el 72% de la variación. Este es un porcentaje aceptablemente grande.

Un método alternativo para determinar el número de componentes principales es mirar un diagrama de pantalla, que es el diagrama de valores propios ordenados de mayor a menor. El número de componentes se determina en el punto, más allá del cual los valores propios restantes son relativamente pequeños y de tamaño comparable (Jollife 2002, Peres-Neto, Jackson y Somers (2005)).

El diagrama de pantalla se puede generar utilizando la función fviz_eig () o fviz_screeplot () [paquete factoextra].



```{r cars4}
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))
```




```{r acp5}

var <- get_pca_var(res.pca)
var
# Coordinates
head(var$coord)
# Cos2: quality on the factore map
head(var$cos2)
# Contributions to the principal components
head(var$contrib)

# Coordinates of variables
head(var$coord, 4)

fviz_pca_var(res.pca, col.var = "black")


```


```{r pressure, echo=FALSE}

```


